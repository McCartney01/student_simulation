import json
from retry import retry
import os
from openai import OpenAI
import re

class Node:
    def __init__(self, level=0, value=-1, answer="", eval=""):
        self.level = level
        self.value = value
        self.answer = answer
        self.eval = eval
    
    def print_node(self):
        print(self.level)
        print(self.value)
        print(self.answer)
        print(self.eval)

prompt = {}
prompt['generate_initial_code'] = """Given a Python programming problem, please write a piece of code to solve it in the style of a Python beginner. Do not include comments in the code. Use the following example as a reference. Directly output your code.

-Example-
Question:
{eg_question}

Code:
{eg_code}

-Real Data-
Question:
{question}

Code:
"""

prompt['evaluate_1'] = """Given a Python programming question, a solution code, along with an error description. For each error in the error description, you need to analyze whether the solution code naturally replicates the error.

Question:
{question}

Code:
{code}

Error Description:
{error_desc}
"""

prompt['evaluate_2'] = """Based on your analysis, rate a score about whether the code aligns with the error description. The score is ranged from 0 to 1, higher score indicates that the code better and more naturally replicates the errors mentioned in the error description. Then give a explanation within 3 sentences.

Output format: (Score | Explanation). For example, (0.1 | The code does not replicate any errors mentioned in the description), or (0.9 | The code successfully reproduces all the errors in the description).
"""


prompt['refine'] = """Imagine there is a code generator designed to simulate a specific student solving Python programming problems. The code generator has access to the student’s past Python code as a reference for their coding style. Additionally, it only knows the descriptions of errors present in the student’s current solution but does not have access to the student’s actual solution code. Based on this information, the code generator pretends to be the student and writes code to complete the problem.

However, the quality of the code generated by the code generator may not be enough. The code might not accurately reflect and replicate the errors described in the error descriptions, or it might not align with the coding style observed in the student’s past code. You will be given an evaluation about the quality of the generated code.

Your task is to revise the generated code based on the provided feedback, ensuring it better meets the requirements. Specifically, the revised code should more accurately replicate the errors described in the error feedback and align more closely with the coding style observed in the student’s past work.

Directly output your revised code without any words or explanations!!

-Real Data-
Student's Past Code:
{eg_code}

New Programming Problem:
{question}

Error Descriptions:
{error_desc}

Code From Generator:
{code}

Evaluation:
{evaluation}

Your Revised Code:
"""

@retry()
def generate(data):
    finished = False
    if os.path.isfile(data["output_file"]):
        with open(data["output_file"]) as f:
            for i in f.readlines():
                i = json.loads(i)
                if i['id'] == data["id"]:
                    finished = True
                    output = i["output"]
                    break
    
    if finished:
        return output
    
    
    client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), timeout=60)

    if "generation_config" in data:
        stream = client.chat.completions.create(
            model=data["model"],
            messages=data["messages"],
            **data["generation_config"]
        ).to_dict()
    else:
        stream = client.chat.completions.create(
            model=data["model"],
            messages=data["messages"],
        ).to_dict()
    
    output_data = {
        "id": data["id"],
        "output": stream,
        "input": data
    }
    with open(data["output_file"], 'a') as f:
        f.write(json.dumps(output_data, ensure_ascii=False)+'\n')
    
    return output_data["output"]
    
def find_value(input_string):
    pattern = r'\(\s*(0(\.\d+)?|1(\.0+)?)\s*\|\s*(.+?)\s*\)'
    matches = re.findall(pattern, input_string)

    values, evals = [], []
    for m in matches:
        pattern = re.compile(r'\-?\d+\.\d+|\-?\d+')
        tmp_score = pattern.findall(m[0])
        value = float(tmp_score[0])
        values.append(value)
        
        text = '|'.join(input_string.split('|')[1:])
        text = ')'.join(text.split(')')[:-1]).strip()

        evals.append(text)
    return values, evals

def evaluate(mid_file, question, code, error_desc, iter, answer_idx, used_model):
    content = prompt['evaluate_1'].format(question=question, code=code, error_desc=error_desc)
    data = {
            "id": f"iter_{iter}_answer_{answer_idx}_value_1",
            "model": used_model,
            "messages": [{"role": "user", "content": content}],
            "output_file": mid_file,
        }
    output = generate(data)['choices'][0]['message']['content']

    content2 = prompt['evaluate_2'].format(error_desc=error_desc)
    data = {
            "id": f"iter_{iter}_answer_{answer_idx}_value_2",
            "model": used_model,
            "messages": [{"role": "user", "content": content},
                         {"role": "assistant", "content": output},
                         {"role": "user", "content": content2}],
            "output_file": mid_file,
        }
    while True:
        output2 = generate(data)['choices'][0]['message']['content']
        values, evals = find_value(output2)
        if len(values)>=1:
            break
        else:
            output2 = output2 + ')'
            values, evals = find_value(output2)
            if len(values)==1:
                break
            print(output2)
            tmp = []
            with open(mid_file) as f:
                for i in f.readlines():
                    i = json.loads(i)
                    if i['id'] != f"iter_{iter}_answer_{answer_idx}_value_2":
                        tmp.append(i)
            with open(mid_file, 'w') as f:
                for i in tmp:
                    line = json.dumps(i, ensure_ascii=False)
                    f.write(line+'\n')

    return values[0], evals[0]

def solve(student_id, problem, example, max_iter=3, max_beam=2, subdir=None, used_model=None, str_model=None):
    mid_dir = f'data_{str_model}/{student_id}/{subdir}_ours/depth_{max_iter}_beam_{max_beam}'
    os.makedirs(mid_dir, exist_ok=True)
    mid_file = f'{mid_dir}/{problem["problem_id"]}.json'
    content = prompt['generate_initial_code'].format(eg_question=example['question'], eg_code=example['program'], question=problem['question'])
    data = {
            "id": "iter_0_generate_code",
            "model": used_model,
            "messages": [{"role": "user", "content": content}],
            "output_file": mid_file,
        }
    output = generate(data)

    answer = output['choices'][0]['message']['content']

    value, eval = evaluate(mid_file=mid_file, question=problem['question'], code=answer, error_desc=problem['generate_error_desc'], iter=0, answer_idx=0, used_model=used_model)

    Nodes_list = []
    root_node = Node(level=0, value=value, answer=answer, eval=eval)
    Nodes_list.append(root_node)
    current_node = root_node
    for i in range(max_iter):
        if current_node.value >= 0.9:
            break
        content = prompt["refine"].format(eg_code=example['program'], question=problem['question'], error_desc=problem['generate_error_desc'], code=current_node.answer, evaluation=current_node.eval)
        data = {
                "id": f"iter_{i+1}_refine",
                "model": used_model,
                "messages": [{"role": "user", "content": content}],
                "output_file": mid_file,
                "generation_config": {"n": max_beam}
            }
        output = generate(data)

        refined_code = [j['message']['content'] for j in output['choices']]

        assert len(refined_code)==max_beam

        nodes = []
        for j, c in enumerate(refined_code):
            value, eval = evaluate(mid_file=mid_file, question=problem['question'], code=c, error_desc=problem['generate_error_desc'], iter=i+1, answer_idx=j, used_model=used_model)
            nodes.append(Node(level=i+1, value=value, answer=c, eval=eval))
        nodes = sorted(nodes, key=(lambda x: x.value), reverse=True)
        Nodes_list += nodes
        current_node = nodes[0]

    return problem['problem_id'], current_node, Nodes_list, student_id